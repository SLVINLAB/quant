head(string)
unlist(data_list)
Simplepos22(unlist(data_list))
SimplePos22(unlist(data_list))
string = paste(unlist(SimplePos22(unlist(data_list))))
head(string)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
library(tm)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
View(noun)
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="cp949"
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)
findFreqTerms(myTdmNC, lowfreq=10)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun
noun = Corpus(VectorSource(noun)) #tm
noun
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
tdm <- TermDocumentMatrix(noun, control=list(tokenize = ko.words, removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
tdm <- TermDocumentMatrix(noun, control=list(tokenize = ko.words, removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
tdm <- TermDocumentMatrix(noun, control=list(removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
tdm
findFreqTerms(tdm, lowfreq=10)
findFreqTerms(tdm, lowfreq=10)
tdm <- TermDocumentMatrix(noun, control=list(tokenize=konlp_tokenizer,
weighting = function(x) weightTfIdf(x, TRUE),
wordLengths=c(1,Inf)))
library(KoNLP)
tdm$dimnames$
Encoding(tdm$dimnames$Terms) = 'UTF-8'
tdm <- TermDocumentMatrix(noun, control=list(removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
tdm$dimnames$
Encoding(tdm$dimnames$Terms) = 'UTF-8'
tdm = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(tdm$dimnames$Terms) = 'UTF-8'
findFreqTerms(tdm, lowfreq=10)
findFreqTerms(tdm, highfreq=10)
tdm <- TermDocumentMatrix(noun, control=list(tokenize = ko.words, removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="cp949"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="cp949"
findFreqTerms(myTdmNC, lowfreq=10)
findFreqTerms(myTdmNC, lowfreq=10)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
View(mtNC)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
mtrowNC<-rowSums(mtNC)
mtNC.order<-mtrowNC[order(mtrowNC,decreasing=T)]
freq.wordsNC<-sample(mtNC.order[mtNC.order>30],25)
freq.wordsNC<-as.matrix(freq.wordsNC)
freq.wordsNC
co.matrix<-freq.wordsNC %*% t(freq.wordsNC)
qgraph(co.matrix,
labels=rownames(co.matrix),
diag=FALSE,
layout='spring',
vsize=log(diag(co.matrix)*2))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
useNIADic(which_dic = c("woorimalsam", "insighter"), category_dic_nms = "economic", backup = T)
setwd('C:/Users/Jin/Desktop/Revenue Surprises and Bond/Data')
data_list = list()
stock_list = list()
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
# Q-graph
string = paste(unlist(SimplePos22(unlist(data_list))))
head(string)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
Encoding(noun)
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
encoding(noun$`1`)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
sessionInfo()
sessionInfo()
sessionInfo(package = NULL)
noun
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
noun
noun[1]
noun[[[1]]]
noun[[1]]
noun = Corpus(noun) #tm
noun = Corpus(VectorSource(noun)) #tm
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(noun) #tm
noun = Corpus(VectorSource(noun) #tm
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
options(mc.cores=1)
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'UTF-8') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
Encoding(myTdmNC$dimnames$Terms)= "cp-949"
Encoding(myTdmNC$dimnames$Terms)= "cp949"
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
mtNC
mtrowNC<-rowSums(mtNC)
mtrowNC
colnames(mtrowNC)
mtrowNC
View(mtrowNC)
rownames(mtrowNC)
mtrowNC[1]
mtrowNC[2]
a = as.data.frame(mtrowNC)
a
rownames(a)
Encoding(rownames(a))
Encoding(rownames(a)) = "UTF-8"
rownames()
rownames(a)
check.for.updates.R()
install.packages('installr')
require(installr)
check.for.updates.R()
install.R()
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
useNIADic(which_dic = c("woorimalsam", "insighter"), category_dic_nms = "economic", backup = T)
setwd('C:/Users/Jin/Desktop/Revenue Surprises and Bond/Data')
data_list = list()
stock_list = list()
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
st))))
# Q-graph
string = paste(unlist(SimplePos22(unlist(data_list))))
head(string)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
Encoding(noun)
myTdmNC<-TermDocumentMatrix(noun)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
mtrowNC<-rowSums(mtNC)
mtNC.order<-mtrowNC[order(mtrowNC,decreasing=T)]
freq.wordsNC<-sample(mtNC.order[mtNC.order>30],25)
freq.wordsNC<-as.matrix(freq.wordsNC)
freq.wordsNC
co.matrix<-freq.wordsNC %*% t(freq.wordsNC)
qgraph(co.matrix,
labels=rownames(co.matrix),
diag=FALSE,
layout='spring',
vsize=log(diag(co.matrix)*2))
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
useNIADic(which_dic = c("woorimalsam", "insighter"), category_dic_nms = "economic", backup = T)
setwd('C:/Users/Jin/Desktop/Revenue Surprises and Bond/Data')
data_list = list()
stock_list = list()
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
for (j in 1:300) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
# Word Cloud
rawword = sapply(data_list, extractNoun, USE.NAMES = F) %>% unlist()
rawword[nchar(rawword)==1] = NA
rawdata = table(rawword)
head(sort(rawdata, decreasing = TRUE), 40)
rawdata = as.data.frame(rawdata)
rawdata$rawword = as.character(rawdata$rawword)
str(rawdata)
pal = brewer.pal(12,"Paired")
wordcloud(words = rawdata$rawword,  # 단어
freq = rawdata$Freq,   # 빈도
min.freq = 3,          # 최소 단어 빈도
max.words = 200,       # 표현 단어 수
random.order = F,      # 고빈도 단어 중앙 배치
rot.per = .1,          # 회전 단어 비율
scale = c(8, 1),     # 단어 크기 범위
colors = pal)          # 색깔 목록
rm(list=ls())
install.packages("sas7bat")
install.packages("sas7bbat")
install.packages("sas7bdat")
library(sas7bdat)
sas = read.sas7bdat(file.choose())
View(sas)
ggplot()+
layer(data = iris,
mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species),
geom = 'point',
stat = 'identity',
position = 'identity')
[출처] 3주일차: 데이터 시각화 ( ggplot2 in R )|작성자 오호힛
library(ggplot2)
library(ggplot2)
ggplot()+
layer(data = iris,
mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species),
geom = 'point',
stat = 'identity',
position = 'identity')
ggplot()+
layer(data = iris,
mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species),
geom = 'point',
stat = 'identity',
position = 'identity')
ggplot(data = iris) +
geom_point(mapping = aes(x=Sepal.Length,y=Sepal.Width,color =  Species),
stat='identity',position='identity')
ggplot(data=iris) +
geom_point(mapping = aes(x=Sepal.Length,y=Sepal.Width))
ggplot(data=iris) +
geom_histogram(mapping = aes(x=Sepal.Length))
ggplot(data=iris) +
geom_boxplot(mapping = aes(x=Species,y=Sepal.Width))
ggplot(data = iris) +
geom_point(mapping = aes(x=Sepal.Length,y=Sepal.Width, color =  Species , shape = 'c' , size = 2)
)
ggplot(data=iris) +
geom_histogram(mapping = aes(x=Sepal.Length,fill=species))
# 예시
ggplot(data = iris,mapping = aes(x=Sepal.Length,y=Sepal.Width,color =  Species)) +
geom_point() +
geom_smooth()
# 참고로 mapping을 point에서 plot으로 위치를 이동했는데 이래도 그림은 똑같고, smooth는 point안에
# aes 내용이 있을 경우 작동하지 않고 그 이유는 나도 모르겠다. 근데 아래와 같이 치면은 작동함.
# 참고로 실행이 안됨. 아마 smooth도 geom이라서 그런거 같음
# 그래서 여기에도 데이터 집어 넣으면 되는지 확인!!!
ggplot(data = iris) +
geom_point(mapping = aes(x=Sepal.Length,y=Sepal.Width,color =  Species)) +
geom_smooth()
# 이 부분은 공부가 필요한데 아래처럼 쓴 경우는 정해진 통계량을 쓸 경우이고, 그렇지 않을 경우
# stat = "----"을 추가해서 사용함 (?)
install.packages("gridExtra")
library(gridExtra)
p7 <- ggplot(data=iris, aes(x=Sepal.Length)) + geom_histogram(aes(y=..count..))
p7 <- ggplot(data=iris, aes(x=Sepal.Length)) + stat_bin()
p8 <- ggplot(data=iris, aes(x=Sepal.Length)) + geom_histogram(aes(y=..density..))
p9 <- ggplot(data=iris, aes(x=Sepal.Length)) + geom_histogram(aes(y=..ncount..))
p10 <- ggplot(data=iris, aes(x=Sepal.Length)) + geom_histogram(aes(y=..ndensity..))
grid.arrange(p7,p8,p9,p10,nrow=2)
ggplot(data=iris, aes(Species,Sepal.Width)) + geom_point() +
stat_summary(geom = "point", fun.y = "mean", colour = "red" , size = 3)
library(dplyr)
library(PerformanceAnalytics)
install.packages("jsonlite")
library(jsonlite)
rm(list=ls())
library(dplyr)
library(PerformanceAnalytics)
library(jsonlite)
setwd('C:/Users/Jin/Desktop/Project/Fnguide_api')
rm(list=ls())
library(dplyr)
library(tidyr)
library(PerformanceAnalytics)
library(jsonlite)
setwd('C:/Users/Jin/Desktop/Project/Fnguide_api')
df_api <- fromJSON("https://api.github.com/users/hadley/repos")
View(df_api)
rm(list=ls())
library(dplyr)
library(tidyr)
library(PerformanceAnalytics)
library(jsonlite)
setwd('C:/Users/Jin/Desktop/Project/Fnguide_api')
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=A005930&fr_dt=20171201&to_dt=20171210")
View(fn_dsf)
fn_dsf = unlist(fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=A005930&fr_dt=20171201&to_dt=20171210"))
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=A005930&fr_dt=20171201&to_dt=20171210") %>% unlist() %>% data.frame()
View(fn_dsf)
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=A005930&fr_dt=20171201&to_dt=20171210")
fn_dsf[[1]]
data = as.data.frame(fn_dsf[[1]])
View(data)
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171210")
data = as.data.frame(fn_dsf[[1]])
rm(list=ls())
library(dplyr)
library(tidyr)
library(PerformanceAnalytics)
library(jsonlite)
setwd('C:/Users/Jin/Desktop/Project/Fnguide_api')
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171210")
data = data.frame(fn_dsf[[1]])
View(data)
#gicode : 종목코드, 미정시 전종목 데이터 추출
#fr_dt ~ to_dt : 날짜 추출 기간
Sys.time(
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171231")
)
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171231")
Sys.time(
fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171231")
)
system.time(
fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171231")
)
View(data)
str(data)
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171231")
fn_dsf = fromJSON("http://app.fnguide.com/apiex/api/dsf/dsf001?gicode=&fr_dt=20171201&to_dt=20171231")
str(data)
