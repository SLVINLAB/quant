library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
useNIADic(which_dic = c("woorimalsam", "insighter"),backup = T)
setwd('C:/Users/Jin/Desktop/Revenue Surprises and Bond/Data')
data_list = list()
stock_list = list()
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
stock_list[[1]][1]
data_list[[1]][1]
rawword = sapply(data_list, extractNoun, USE.NAMES = F)
View(rawword)
useNIADic(which_dic = c("woorimalsam", "insighter"), category_dic_nms = "economic", backup = T)
rawword = sapply(data_list, extractNoun, USE.NAMES = F) %>% unlist()
rawword[nchar(rawword)==1] = NA
rawdata = table(rawword)
head(sort(rawdata, decreasing = TRUE), 40)
rawdata = as.data.frame(rawdata)
rawdata$rawword = as.character(rawdata$rawword)
str(rawdata)
pal = brewer.pal(12,"Paired")
wordcloud(words = rawdata$rawword,  # 단어
freq = rawdata$Freq,   # 빈도
min.freq = 3,          # 최소 단어 빈도
max.words = 200,       # 표현 단어 수
random.order = F,      # 고빈도 단어 중앙 배치
rot.per = .1,          # 회전 단어 비율
scale = c(8, 1),     # 단어 크기 범위
colors = pal)          # 색깔 목록
string = paste(unlist(SimplePos22(data_list)))
head(string)
unlist(data_list)
Simplepos22(unlist(data_list))
SimplePos22(unlist(data_list))
string = paste(unlist(SimplePos22(unlist(data_list))))
head(string)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
library(tm)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
View(noun)
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="cp949"
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
myTdmNC = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)
findFreqTerms(myTdmNC, lowfreq=10)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun
noun = Corpus(VectorSource(noun)) #tm
noun
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
tdm <- TermDocumentMatrix(noun, control=list(tokenize = ko.words, removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
tdm <- TermDocumentMatrix(noun, control=list(tokenize = ko.words, removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
tdm <- TermDocumentMatrix(noun, control=list(removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
tdm
findFreqTerms(tdm, lowfreq=10)
findFreqTerms(tdm, lowfreq=10)
tdm <- TermDocumentMatrix(noun, control=list(tokenize=konlp_tokenizer,
weighting = function(x) weightTfIdf(x, TRUE),
wordLengths=c(1,Inf)))
library(KoNLP)
tdm$dimnames$
Encoding(tdm$dimnames$Terms) = 'UTF-8'
tdm <- TermDocumentMatrix(noun, control=list(removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
tdm$dimnames$
Encoding(tdm$dimnames$Terms) = 'UTF-8'
tdm = TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(tdm$dimnames$Terms) = 'UTF-8'
findFreqTerms(tdm, lowfreq=10)
findFreqTerms(tdm, highfreq=10)
tdm <- TermDocumentMatrix(noun, control=list(tokenize = ko.words, removePunctuation=T, removeNumbers=T, wordLengths=c(2, Inf)))
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="cp949"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="cp949"
findFreqTerms(myTdmNC, lowfreq=10)
findFreqTerms(myTdmNC, lowfreq=10)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
View(mtNC)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
mtrowNC<-rowSums(mtNC)
mtNC.order<-mtrowNC[order(mtrowNC,decreasing=T)]
freq.wordsNC<-sample(mtNC.order[mtNC.order>30],25)
freq.wordsNC<-as.matrix(freq.wordsNC)
freq.wordsNC
co.matrix<-freq.wordsNC %*% t(freq.wordsNC)
qgraph(co.matrix,
labels=rownames(co.matrix),
diag=FALSE,
layout='spring',
vsize=log(diag(co.matrix)*2))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
install.packages(c("BH", "bindr", "blob", "DBI", "ddalpha", "devtools", "digest", "DRR", "htmlTable", "htmlwidgets", "igraph", "irlba", "keras", "knitr", "lava", "lubridate", "openssl", "PerformanceAnalytics", "pillar", "plogr", "quantreg", "randomForest", "Rcpp", "RcppEigen", "recipes", "reticulate", "rlang", "rmarkdown", "Rttf2pt1", "selectr", "sfsmisc", "stringi", "stringr", "tensorflow", "tfruns", "tibble", "tidyr", "tidyselect", "timeDate", "TTR", "viridis", "viridisLite", "withr", "xml2", "xts", "yaml", "zeallot", "zoo"))
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
useNIADic(which_dic = c("woorimalsam", "insighter"), category_dic_nms = "economic", backup = T)
setwd('C:/Users/Jin/Desktop/Revenue Surprises and Bond/Data')
data_list = list()
stock_list = list()
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
# Q-graph
string = paste(unlist(SimplePos22(unlist(data_list))))
head(string)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
Encoding(noun)
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
findFreqTerms(myTdmNC, lowfreq=10)
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(wordLengths=c(4,10),
removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
encoding(noun$`1`)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)="UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
sessionInfo()
sessionInfo()
sessionInfo(package = NULL)
noun
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
noun
noun[1]
noun[[[1]]]
noun[[1]]
noun = Corpus(noun) #tm
noun = Corpus(VectorSource(noun)) #tm
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(noun) #tm
noun = Corpus(VectorSource(noun) #tm
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
options(mc.cores=1)
options(mc.cores=1)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'UTF-8') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
Encoding(myTdmNC$dimnames$Terms)= "cp-949"
Encoding(myTdmNC$dimnames$Terms)= "cp949"
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
mtNC
mtrowNC<-rowSums(mtNC)
mtrowNC
colnames(mtrowNC)
mtrowNC
View(mtrowNC)
rownames(mtrowNC)
mtrowNC[1]
mtrowNC[2]
a = as.data.frame(mtrowNC)
a
rownames(a)
Encoding(rownames(a))
Encoding(rownames(a)) = "UTF-8"
rownames()
rownames(a)
check.for.updates.R()
install.packages('installr')
require(installr)
check.for.updates.R()
install.R()
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
useNIADic(which_dic = c("woorimalsam", "insighter"), category_dic_nms = "economic", backup = T)
setwd('C:/Users/Jin/Desktop/Revenue Surprises and Bond/Data')
data_list = list()
stock_list = list()
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
st))))
# Q-graph
string = paste(unlist(SimplePos22(unlist(data_list))))
head(string)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
noun = Corpus(VectorSource(noun)) #tm
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)
# NOUN
noun = str_match_all(string, "[가-힣]+/[N][C]|[가-힣]+/[N][Q]+")%>% unlist()
noun = str_replace_all(noun,"/[N][C]","") %>% str_replace_all("/[N][Q]","") %>%unlist()
Encoding(noun)
myTdmNC<-TermDocumentMatrix(noun)
myTdmNC<-TermDocumentMatrix(noun,control = list(removePunctuation=T,removeNumbers=T,weighting=weightBin))
Encoding(myTdmNC$dimnames$Terms)= "UTF-8"
findFreqTerms(myTdmNC, lowfreq=10)
# 제대로 작동했다면 extractnoun함수를 통해 얻은 결과값과 대충 비슷한 형태의 결과가 나온다.
mtNC<-as.matrix(myTdmNC) #행렬(matrix)로 변환하는 게 상관성 분석의 핵심이다.
mtrowNC<-rowSums(mtNC)
mtNC.order<-mtrowNC[order(mtrowNC,decreasing=T)]
freq.wordsNC<-sample(mtNC.order[mtNC.order>30],25)
freq.wordsNC<-as.matrix(freq.wordsNC)
freq.wordsNC
co.matrix<-freq.wordsNC %*% t(freq.wordsNC)
qgraph(co.matrix,
labels=rownames(co.matrix),
diag=FALSE,
layout='spring',
vsize=log(diag(co.matrix)*2))
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(KoNLP)
library(knitr)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(qgraph)
useNIADic(which_dic = c("woorimalsam", "insighter"), category_dic_nms = "economic", backup = T)
setwd('C:/Users/Jin/Desktop/Revenue Surprises and Bond/Data')
data_list = list()
stock_list = list()
for (j in 1:100) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
for (j in 1:300) {
url_news = 'http://finance.naver.com/research/company_list.nhn?&page='
url_news = paste0(url_news, j)
news_data = read_html(url_news, encoding = 'cp949') #UTF-8
# news_data
head = news_data %>% html_nodes('div#newarea') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a')  %>% html_text()
#head
data = head[1:(length(head)-17)]
data[data==""] = NA
data = na.omit(data)
stock = data[seq(from = 1, to = length(data), by=2 )]
head_report = data[seq(from = 2, to = length(data), by=2 )]
head_report = str_trim(head_report)
head_report
data_list[[j]] = head_report
stock_list[[j]] = stock
}
# Word Cloud
rawword = sapply(data_list, extractNoun, USE.NAMES = F) %>% unlist()
rawword[nchar(rawword)==1] = NA
rawdata = table(rawword)
head(sort(rawdata, decreasing = TRUE), 40)
rawdata = as.data.frame(rawdata)
rawdata$rawword = as.character(rawdata$rawword)
str(rawdata)
pal = brewer.pal(12,"Paired")
wordcloud(words = rawdata$rawword,  # 단어
freq = rawdata$Freq,   # 빈도
min.freq = 3,          # 최소 단어 빈도
max.words = 200,       # 표현 단어 수
random.order = F,      # 고빈도 단어 중앙 배치
rot.per = .1,          # 회전 단어 비율
scale = c(8, 1),     # 단어 크기 범위
colors = pal)          # 색깔 목록
rm(list=ls())
install.packages("sas7bat")
install.packages("sas7bbat")
install.packages("sas7bdat")
library(sas7bdat)
sas = read.sas7bdat(file.choose())
View(sas)
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(knitr)
library(slackr)
setwd('C:/Users/Jin/Desktop/Project/Slackopenapi')
url_news = 'http://dart.fss.or.kr/dsac001/mainY.do'
keyword = "주주"
slackr_setup(api_token = "xoxb-338810061686-2LQYAryh59sZKuSbQj0kxTFS", username = "Jooji")
crawling_function = function(url_news){
repeat {
news_data = read_html(url_news, encoding = 'UTF-8') #UTF-8
head = news_data %>% html_nodes('div.table_list') %>% html_nodes("table") %>% html_nodes("tr")%>% html_nodes("td") %>%  html_text()
head = str_trim(head)
head
news = data.frame(time = character(0), stock = character(0), head = character(0), date = character(0))
for (i in 0:((length(head)/6)-1)) {
news = bind_rows(news, data.frame(time = head[1+6*i], stock = head[2+6*i], head = head[3+6*i], date = head[5+6*i]))
}
print(news)
if(length(grep(keyword, news$head)) > 0){
print(news[grep(keyword, news$head),])
news = news[grep(keyword, news$head),]
for (j in 1:nrow(news)) {
text = paste(as.character(news[j,])[1], as.character(news[j,])[2], as.character(news[j,])[3], as.character(news[j,])[4])
text_slackr(text, channel = '#dart')
}}
Sys.sleep(10)
}
}
crawling_function(url_news)
rm(list=ls())
library(rvest)
library(stringr)
library(dplyr)
library(tidyr)
library(knitr)
library(slackr)
setwd('C:/Users/Jin/Desktop/Project/Slackopenapi')
url_news = 'http://dart.fss.or.kr/dsac001/mainY.do'
keyword = "주주"
slackr_setup(api_token = "xoxb-338810061686-2LQYAryh59sZKuSbQj0kxTFS", username = "Jooji")
crawling_function = function(url_news){
repeat {
news_data = read_html(url_news, encoding = 'UTF-8') #UTF-8
head = news_data %>% html_nodes('div.table_list') %>% html_nodes("table") %>% html_nodes("tr")%>% html_nodes("td") %>%  html_text()
head = str_trim(head)
head
news = data.frame(time = character(0), stock = character(0), head = character(0), date = character(0))
for (i in 0:((length(head)/6)-1)) {
news = bind_rows(news, data.frame(time = head[1+6*i], stock = head[2+6*i], head = head[3+6*i], date = head[5+6*i]))
}
print(news)
if(length(grep(keyword, news$head)) > 0){
print(news[grep(keyword, news$head),])
news = news[grep(keyword, news$head),]
for (j in 1:nrow(news)) {
text = paste(as.character(news[j,])[1], as.character(news[j,])[2], as.character(news[j,])[3], as.character(news[j,])[4])
text_slackr(text, channel = '#dart')
}}
Sys.sleep(10)
}
}
crawling_function(url_news)
